{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il nome DSC_3650.JPG è presente nel file Excel.\n",
      "Il nome DSC_4595.JPG è presente nel file Excel.\n",
      "Il nome Immagine 031.jpg è presente nel file Excel.\n",
      "Il nome DSCN2551.jpg è presente nel file Excel.\n",
      "Il nome DSCN9127.jpg è presente nel file Excel.\n",
      "Il numero di immagini ancora presenti nel file Excel è: 5.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il file Excel\n",
    "rankings_df = pd.read_excel('../features/RANKINGS.xlsx', sheet_name='CBR FINAL', header=None)\n",
    "\n",
    "# Estrai i nomi delle colonne a partire dalla terza colonna\n",
    "start_column = 2  # Indice della terza colonna\n",
    "rankings_columns = rankings_df.iloc[0, start_column:].tolist()\n",
    "\n",
    "# Aggiungi i nomi dalla colonna A a partire dalla riga 4\n",
    "#column_A_names = rankings_df.iloc[3:, 0].tolist()\n",
    "#rankings_columns.extend(column_A_names)\n",
    "\n",
    "# Carica il file CSV\n",
    "classifier_df = pd.read_csv('../features/convnext_small_classifier.csv')\n",
    "\n",
    "# Estrai i nomi dalla colonna 'name'\n",
    "classifier_names = classifier_df['name'].tolist()\n",
    "\n",
    "immagini_mancanti = 0\n",
    "immagini_presenti = 0\n",
    "\n",
    "# Confronta i valori delle celle\n",
    "for name in classifier_names:\n",
    "    if name in rankings_columns:\n",
    "        print(f\"Il nome {name} è presente nel file Excel.\")\n",
    "        immagini_presenti += 1\n",
    "\n",
    "print(f\"Il numero di immagini ancora presenti nel file Excel è: {immagini_presenti}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_casi', 'TIPO DI ULCERA', 'Immagine 031.jpg', '11-04-2008 015.jpg',\n",
      "       'Immagine 379.jpg', 'DSCN2162.JPG', 'DSCN2465.jpg', 'DSCN2549.jpg',\n",
      "       'DSCN2551.jpg', 'DSCN2702.JPG', 'DSCN9127.jpg', '4-06 033.jpg',\n",
      "       'DSC_1888.JPG', 'DSC_5396.JPG', 'Immagine 002.jpg', 'DSCN2477.jpg',\n",
      "       'DSC_6553.JPG', 'DSC_6482.JPG', 'DSCN5480.JPG', 'DSC_2957.JPG',\n",
      "       'DSC_0610.JPG', '893019313d524274b48aa1f318e6de5f.JPG',\n",
      "       '76d46b73787fbf1b746d9e5c717d0612.JPG', 'DSC_3645.JPG', 'DSC_3650.JPG',\n",
      "       'DSC_9556.JPG', 'DSCN8419.jpg', 'Immagine 0221.jpg', 'Immagine 213.jpg',\n",
      "       'imm 041.jpg', 'DSC_4595.JPG', 'Immagine 025.jpg',\n",
      "       '9af90f97dbf493dacb7bd83d66d420d1.jpg'],\n",
      "      dtype='object')\n",
      "['Immagine 010.jpg', 'DSC_7807.JPG', 'DSC_3261.JPG', 'DSC_9459.JPG', 'DSC_9220.JPG', 'DSC_7944.JPG', '0a2ab38bc6b1741e18c7d59319356e58.jpg', '28-05-2008 016.jpg', 'DSC_2974.JPG', 'DSCN8419 .jpg', 'Immagine 096.jpg', 'Immagine 247.jpg', 'DSC_3650.JPG', '3ac3a26a6d56e997d744e2cdfbbe9737.jpg', '40ec07a95ef7f52db5cc3513455d60cc.jpg', '9f8725243c3b743653350c60f64fb51b.JPG', 'DSCN4287.jpg', 'DSC_8691.JPG', '32c094b9e9020102948f73f52fa520e3.jpg', 'DSC_4595.JPG', 'DSC_4427.JPG', 'DSC_1480.JPG', '4fa81a5a2d2d8bb818c6ffaee409c998.JPG', 'DSC_6415.JPG', '4928c65f3b55b45077757abbd1b7f0c9.jpg', 'Immagine 133.jpg', 'DSCN3981.jpg', 'DSC_3118.JPG', 'DSC_4098.JPG', '65612a5a7fcee55e75e410cdef7924ae.jpg', 'e450c45e930e4199bb431c2b0055fe8a.JPG', 'DSC_9712.JPG', '38f5e82ab7437557bc24807419fcbea9.JPG', 'DSC_0143.JPG', 'fc60f3cf542b7842c1160463eb46e1eb.JPG', '7c0cda16b624d6022a57fb8970364444.JPG', 'DSCN1421.JPG', 'DSC_6517.JPG', 'DSC_81192.JPG', '80b1f09d2095954f038f6150b697369d.JPG', '5d284e411efcb9fa01f2d76217f0b09f.JPG', 'DSC_3473.JPG', '733fbbddaca3f39f66b924158a84f0a4.JPG', 'DSC_6877.JPG', '440caf7ed1178cae6f55c0385c595924.JPG', 'adea3cd26db33dbad396e3849c56a146.jpg', 'ced5b1d1c81346dfa214058737ec02e1.JPG', 'DSC_2830.JPG', 'c6cdc8915aa7d89a2442364690ccd7e2.JPG', 'Immagine 0212.jpg', 'Immagine 180.jpg', '6a5960c78102880d8acc069a2014fb0d.JPG', 'DSC_4068.JPG', '69e6bddd5dc267708fbc2a72bc3704f4.JPG', 'bff8ffa44b18f47f419bc204e06f74bd.JPG', '8bbd869cada25831ec94629aa0530a8a.jpg', 'a232a3456f2fe4d12aa96e1c7e086ee6.JPG', 'DSC_9074.JPG', 'DSCN1633.JPG', 'Immagine 017.jpg', '531027275e99c1f7d8975f748c1bdbc8.jpg', '4f7c41aa5111aa0f6721ec4408221c37.JPG', 'DSCN7757.jpg', 'Immagine 031.jpg', 'DSCN2221.jpg', 'DSCN2551.jpg', 'f95ccd64a6bdbbc4c6b4aab48d8128d0.jpg', 'DSC_4331.JPG', 'DSC_9425.JPG', '836c4e2a02cb4f5b0856f0fd9c17ef80.JPG', '28-05-2008 013.jpg', '6f5a7b21e6cdc5720c4c7f496ce4f2bb.jpg', 'b485225b419488c3d59eb893fb0a9ebd.jpg', '010.JPG', 'DSCN9127.jpg', 'DSC_9674.JPG', 'c447cee36c33c0579c6c08bd0b1db511.jpg', '7bc755d987dea554874af5d47c956022.jpg', 'DSC_8576.JPG', 'DSCN7924.jpg', 'Immagine 108.jpg', 'Immagine 382.jpg', 'DSC_9424.JPG', 'DSC_3507.JPG', 'Foto 27-11-2013 117.jpg', 'DSC_6084.JPG', 'DSC_9699.JPG', 'DSCN3315.JPG', 'DSCN7097.jpg', 'id_casi', 'TIPO DI ULCERA']\n",
      "['DSC_4595.JPG', 'DSCN9127.jpg', 'DSCN2551.jpg', 'DSC_3650.JPG', 'Immagine 031.jpg', 'id_casi', 'TIPO DI ULCERA']\n"
     ]
    }
   ],
   "source": [
    "# salvataggio in un file excel dei casi ancora presenti\n",
    "import pandas as pd\n",
    "\n",
    "# Carica il file Excel\n",
    "rankings_df = pd.read_excel('../features/RANKINGS.xlsx', sheet_name='CBR FINAL')\n",
    "\n",
    "# Estrai i nomi delle colonne a partire dalla prima colonna\n",
    "start_column = 0  # Indice della terza colonna\n",
    "rankings_columns = rankings_df.columns[start_column:]\n",
    "\n",
    "# Carica il file CSV\n",
    "classifier_df = pd.read_csv('../features/convnext_small_classifier.csv')\n",
    "\n",
    "# Estrai i nomi dalla colonna 'name'\n",
    "classifier_names = classifier_df['name'].tolist()\n",
    "# Aggiungi \"id_casi\" alla lista dei nomi\n",
    "classifier_names.append('id_casi')\n",
    "classifier_names.append('TIPO DI ULCERA')\n",
    "\n",
    "print(rankings_columns)\n",
    "print(classifier_names)\n",
    "# Trova i nomi comuni da mantenere\n",
    "common_names = list(set(rankings_columns) & set(classifier_names))\n",
    "print(common_names)\n",
    "# Seleziona solo le colonne del file Excel che corrispondono ai nomi presenti nel file CSV\n",
    "filtered_rankings_df = rankings_df[common_names]\n",
    "\n",
    "# Salva il DataFrame filtrato in un nuovo file Excel\n",
    "filtered_rankings_df.to_excel('../features/filtered_rankings.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il file Excel\n",
    "rankings_df = pd.read_excel('../features/filtered_rankings.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Estrai il contenuto della colonna 'id_casi'\n",
    "ranking_ids = rankings_df['id_casi'].tolist()\n",
    "\n",
    "# Carica il file CSV\n",
    "classifier_df = pd.read_csv('../features/convnext_small_classifier.csv')\n",
    "\n",
    "# Estrai i nomi dalla colonna 'name'\n",
    "classifier_names = classifier_df['name'].tolist()\n",
    "# Aggiungi \"id_casi\" alla lista dei nomi\n",
    "classifier_names.append('id_casi')\n",
    "\n",
    "# Trova i nomi comuni da mantenere\n",
    "common_names = list(set(ranking_ids) & set(classifier_names))\n",
    "\n",
    "# Seleziona solo le righe del file Excel che corrispondono ai nomi presenti nel file CSV\n",
    "filtered_rankings_df = rankings_df[rankings_df['id_casi'].isin(common_names)] \n",
    "\n",
    "# Salva in un file CSV i casi ancora\n",
    "# Seleziona i nomi delle colonne che terminano con jpg o JPG da filtered_rankings_df\n",
    "column_names = filtered_rankings_df.columns[filtered_rankings_df.columns.str.endswith(('jpg', 'JPG'))]\n",
    "first_row_values = filtered_rankings_df.loc[0, column_names].tolist()\n",
    "column_names_df = pd.DataFrame({'CASI ANCORA': column_names, 'TIPO DI ULCERA': first_row_values})\n",
    "# sostituisci i valori della colonna 'TIPO DI ULCERA'\n",
    "column_names_df['TIPO DI ULCERA'] = column_names_df['TIPO DI ULCERA'].replace('NEOPLASTICA', 0)\n",
    "column_names_df['TIPO DI ULCERA'] = column_names_df['TIPO DI ULCERA'].replace('AFTOSA', 1)\n",
    "column_names_df['TIPO DI ULCERA'] = column_names_df['TIPO DI ULCERA'].replace('TRAUMATICA', 2)\n",
    "column_names_df.to_csv('casi_ancora.csv', index=False)\n",
    "\n",
    "# rimuovi la seconda riga\n",
    "filtered_rankings_df = filtered_rankings_df.drop(filtered_rankings_df.index[0])\n",
    "\n",
    "# sostituisci i valori della colonna 'TIPO DI ULCERA'\n",
    "filtered_rankings_df['TIPO DI ULCERA'] = filtered_rankings_df['TIPO DI ULCERA'].replace('NEOPLASTICA', 0)\n",
    "filtered_rankings_df['TIPO DI ULCERA'] = filtered_rankings_df['TIPO DI ULCERA'].replace('AFTOSA', 1)\n",
    "filtered_rankings_df['TIPO DI ULCERA'] = filtered_rankings_df['TIPO DI ULCERA'].replace('TRAUMATICA', 2)\n",
    "\n",
    "# controlla che non ci siano valori che si ripetono nella colonna 'id_casi', se ci sono mantiene solo la prima occorrenza\n",
    "filtered_rankings_df = filtered_rankings_df.drop_duplicates(subset='id_casi', keep='first')\n",
    "\n",
    "# Rimuovi i duplicati da ogni riga mantenendo solo la prima occorrenza di ciascun valore,\n",
    "# escludendo le colonne \"id_casi\" e \"TIPO DI ULCERA\"\n",
    "for index, row in filtered_rankings_df.iterrows():\n",
    "    columns_to_check = row.index.difference(['id_casi', 'TIPO DI ULCERA'])\n",
    "    filtered_rankings_df.loc[index, columns_to_check] = row[columns_to_check].drop_duplicates(keep='first')\n",
    "\n",
    "# Rimuovi le righe che contengono solo valori nulli\n",
    "for index, row in filtered_rankings_df.iterrows():\n",
    "    columns_to_check = row.index.difference(['id_casi', 'TIPO DI ULCERA'])\n",
    "    if row[columns_to_check].isnull().all():\n",
    "        filtered_rankings_df.drop(index, inplace=True)\n",
    "\n",
    "# Salva il DataFrame filtrato in un nuovo file CSV\n",
    "filtered_rankings_df.to_csv('../features/final_ranking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i due file CSV\n",
    "df1 = pd.read_csv('../features/convnext_small_classifier.csv')\n",
    "df2 = pd.read_csv('../features/final_ranking.csv')\n",
    "df3 = pd.read_csv('../features/casi_ancora.csv')\n",
    "\n",
    "# Esegui il merge basato sulla colonna comune\n",
    "merged_df1 = pd.merge(df1, df2, left_on='name', right_on='id_casi', how='inner')\n",
    "merged_df2 = pd.merge(df1, df3, left_on='name', right_on='CASI ANCORA', how='inner')\n",
    "\n",
    "# Unisci i due DataFrame\n",
    "merged_df = pd.concat([merged_df1, merged_df2])\n",
    "\n",
    "#sostituisci le celle vuote con il valore 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['id_casi'])\n",
    "merged_df = merged_df.drop(columns=['CASI ANCORA'])\n",
    "merged_df = merged_df.drop(columns=['TIPO DI ULCERA'])\n",
    "\n",
    "# Salva il DataFrame unito in un nuovo file CSV\n",
    "merged_df.to_csv('../features/merged_convnext_small_classifier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i due file CSV\n",
    "df1 = pd.read_csv('../features/squeezenet1_0_classifier.csv')\n",
    "df2 = pd.read_csv('../features/final_ranking.csv')\n",
    "df3 = pd.read_csv('../features/casi_ancora.csv')\n",
    "\n",
    "# Esegui il merge basato sulla colonna comune\n",
    "merged_df1 = pd.merge(df1, df2, left_on='name', right_on='id_casi', how='inner')\n",
    "merged_df2 = pd.merge(df1, df3, left_on='name', right_on='CASI ANCORA', how='inner')\n",
    "\n",
    "# Unisci i due DataFrame\n",
    "merged_df = pd.concat([merged_df1, merged_df2])\n",
    "\n",
    "#sostituisci le celle vuote con il valore 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['id_casi'])\n",
    "merged_df = merged_df.drop(columns=['CASI ANCORA'])\n",
    "merged_df = merged_df.drop(columns=['TIPO DI ULCERA'])\n",
    "\n",
    "# Salva il DataFrame unito in un nuovo file CSV\n",
    "merged_df.to_csv('../features/merged_squeezenet1_0_classifier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i due file CSV\n",
    "df1 = pd.read_csv('../features/swin_s_head.csv')\n",
    "df2 = pd.read_csv('../features/final_ranking.csv')\n",
    "df3 = pd.read_csv('../features/casi_ancora.csv')\n",
    "\n",
    "# Esegui il merge basato sulla colonna comune\n",
    "merged_df1 = pd.merge(df1, df2, left_on='name', right_on='id_casi', how='inner')\n",
    "merged_df2 = pd.merge(df1, df3, left_on='name', right_on='CASI ANCORA', how='inner')\n",
    "\n",
    "# Unisci i due DataFrame\n",
    "merged_df = pd.concat([merged_df1, merged_df2])\n",
    "\n",
    "#sostituisci le celle vuote con il valore 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['id_casi'])\n",
    "merged_df = merged_df.drop(columns=['CASI ANCORA'])\n",
    "merged_df = merged_df.drop(columns=['TIPO DI ULCERA'])\n",
    "\n",
    "# Salva il DataFrame unito in un nuovo file CSV\n",
    "merged_df.to_csv('../features/merged_swin_s_head.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i due file CSV\n",
    "df1 = pd.read_csv('../features/vit_b_16_heads.csv')\n",
    "df2 = pd.read_csv('../features/final_ranking.csv')\n",
    "df3 = pd.read_csv('../features/casi_ancora.csv')\n",
    "\n",
    "# Esegui il merge basato sulla colonna comune\n",
    "merged_df1 = pd.merge(df1, df2, left_on='name', right_on='id_casi', how='inner')\n",
    "merged_df2 = pd.merge(df1, df3, left_on='name', right_on='CASI ANCORA', how='inner')\n",
    "\n",
    "# Unisci i due DataFrame\n",
    "merged_df = pd.concat([merged_df1, merged_df2])\n",
    "\n",
    "#sostituisci le celle vuote con il valore 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['id_casi'])\n",
    "merged_df = merged_df.drop(columns=['CASI ANCORA'])\n",
    "merged_df = merged_df.drop(columns=['TIPO DI ULCERA'])\n",
    "\n",
    "# Salva il DataFrame unito in un nuovo file CSV\n",
    "merged_df.to_csv('../features/merged_vit_b_16_heads.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea il file con le triplette per il contrastive learning\n",
    "import pandas as pd\n",
    "\n",
    "# Carica il file CSV\n",
    "df = pd.read_csv('../features/final_ranking.csv')\n",
    "\n",
    "# crea una lista vuota per contenere i DataFrame temporanei\n",
    "triplets_list = []\n",
    "\n",
    "# per ogni riga del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Seleziona solo le prime colonne escludendo le ultime due colonne\n",
    "    row_subset = row.iloc[:-2]\n",
    "    # controlla che nella riga ci sia almeno un valore diverso e uguale a 1 escludendo le ultime due colonne\n",
    "    if row_subset.eq(1).any() & row_subset.ne(1).any():\n",
    "        # utilizza il nome della colonna con valore 1 come positivo escludendo le ultime due colonne\n",
    "        positive = row_subset[row_subset.eq(1)].index[0]\n",
    "        # ogni valore diverso da 1 e diverso da nullo è un negativo escludendo le ultime due colonne\n",
    "        negatives = row_subset[row_subset.ne(1) & row_subset.notnull()].index\n",
    "        # salva il tipo di ulcera\n",
    "        ulcera = row['TIPO DI ULCERA']\n",
    "        # per ogni negativo crea una nuova riga\n",
    "        for negative in negatives:\n",
    "            triplets_list.append({'id_casi': row['id_casi'], 'positive': positive, 'negative': negative, 'TIPO DI ULCERA': ulcera})\n",
    "\n",
    "# Converti la lista di dizionari in un DataFrame\n",
    "triplets_df = pd.DataFrame(triplets_list)\n",
    "\n",
    "# Salva il DataFrame con le triplette in un nuovo file CSV\n",
    "triplets_df.to_csv('../features/triplets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il file excel\n",
    "df = pd.read_excel('../features/RANKINGS.xlsx', sheet_name='CBR FINAL')\n",
    "\n",
    "# Rimuovi la seconda riga\n",
    "df = df.drop([1])\n",
    "\n",
    "# Crea una lista vuota per contenere i DataFrame temporanei\n",
    "triplets_list = []\n",
    "\n",
    "# Mappa i tipi di ulcera a numeri interi\n",
    "ulcera_mapping = {'NEOPLASTICA': 0, 'AFTOSA': 1, 'TRAUMATICA': 2}\n",
    "\n",
    "# Per ogni riga del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Seleziona solo le colonne escludendo la prima colonna\n",
    "    row_subset = row.iloc[1:]\n",
    "    # Converti i valori delle colonne in numeri, ignorando eventuali errori (NaN rimarranno NaN)\n",
    "    row_subset_numeric = pd.to_numeric(row_subset, errors='coerce')\n",
    "    # Controlla che ci siano almeno due valori non nulli\n",
    "    if row_subset_numeric.notnull().sum() >= 2:\n",
    "        # Crea triplette con tutte le possibili combinazioni di positivi e negativi (positivo se valore minore del negativo)\n",
    "        for positive in row_subset_numeric[row_subset_numeric.notnull()].index:\n",
    "            for negative in row_subset_numeric[row_subset_numeric.notnull()].index:\n",
    "                if row_subset_numeric[positive] < row_subset_numeric[negative]:\n",
    "                    # Sostituisci i valori della colonna 'TIPO DI ULCERA' con i numeri interi mappati\n",
    "                    ulcera = ulcera_mapping.get(row['TIPO DI ULCERA'], -1)  # Usa -1 se il tipo di ulcera non è presente nel mapping\n",
    "                    triplets_list.append({'id_casi': row['id_casi'], 'positive': positive, 'negative': negative, 'TIPO DI ULCERA': ulcera})\n",
    "\n",
    "# Converti la lista di dizionari in un DataFrame\n",
    "triplets_df = pd.DataFrame(triplets_list)\n",
    "\n",
    "# Salva il DataFrame con le triplette in un nuovo file CSV\n",
    "triplets_df.to_csv('../features/contrastive_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH ANNOTATIONS AND CATEGORIES\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i dati dal file JSON originale\n",
    "with open(\"../features/dataset.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Leggi il file CSV\n",
    "csv_data = pd.read_csv('../features/contrastive_dataset.csv')\n",
    "\n",
    "# Dizionario per memorizzare i dati combinati\n",
    "combined_data = []\n",
    "\n",
    "# contatore per id delle immagini\n",
    "id_counter = 0\n",
    "\n",
    "# Per ogni riga nel file CSV\n",
    "for index, row in csv_data.iterrows():\n",
    "    # Trova la riga corrispondente nel file JSON utilizzando la corrispondenza tra \"file_name\" ed \"id_casi\"\n",
    "    matching_image = next((image for image in json_data[\"images\"] if image[\"file_name\"] == row[\"id_casi\"]), None)\n",
    "    if matching_image:\n",
    "        # Crea una nuova riga combinando i dati da entrambe le righe\n",
    "        new_entry = {\n",
    "            \"id_contrastive\": id_counter,\n",
    "            \"id\": matching_image[\"id\"],\n",
    "            \"id_casi\": row[\"id_casi\"],\n",
    "            \"positive\": row[\"positive\"],\n",
    "            \"negative\": row[\"negative\"],\n",
    "            \"category_id\": row[\"TIPO DI ULCERA\"],\n",
    "            \"dataset_id\": matching_image[\"dataset_id\"],\n",
    "            \"path\": matching_image[\"path\"],\n",
    "            \"width\": matching_image[\"width\"],\n",
    "            \"height\": matching_image[\"height\"],\n",
    "            \"file_name\": matching_image[\"file_name\"],\n",
    "            \"annotated\": matching_image[\"annotated\"],\n",
    "            \"annotating\": matching_image[\"annotating\"],\n",
    "            \"num_annotations\": matching_image[\"num_annotations\"],\n",
    "            \"metadata\": matching_image[\"metadata\"],\n",
    "            \"deleted\": matching_image[\"deleted\"],\n",
    "            \"milliseconds\": matching_image[\"milliseconds\"],\n",
    "            \"events\": matching_image[\"events\"],\n",
    "            \"regenerate_thumbnail\": matching_image[\"regenerate_thumbnail\"]\n",
    "        }\n",
    "        combined_data.append(new_entry)\n",
    "\n",
    "        # Incrementa il contatore per l'id delle immagini\n",
    "        id_counter += 1\n",
    "\n",
    "# Estrai gli ID delle immagini presenti nel file JSON combinato\n",
    "image_ids = set(image[\"id\"] for image in combined_data)\n",
    "\n",
    "# Dizionario per memorizzare le annotazioni filtrate\n",
    "filtered_annotations = []\n",
    "\n",
    "# Filtra le annotazioni basate sugli ID delle immagini\n",
    "for annotation in json_data[\"annotations\"]:\n",
    "    if annotation[\"image_id\"] in image_ids:\n",
    "        filtered_annotations.append(annotation)\n",
    "\n",
    "# Aggiungi le informazioni \"annotations\" filtrate e \"categories\" dal file JSON originale\n",
    "combined_json = {\n",
    "    \"images\": combined_data,\n",
    "    \"annotations\": filtered_annotations,\n",
    "    \"categories\": json_data[\"categories\"]\n",
    "}\n",
    "\n",
    "# Salva i dati combinati in un nuovo file JSON\n",
    "with open(\"../features/contrastive_dataset.json\", \"w\") as f:\n",
    "    json.dump(combined_json, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT ANNOTATIONS AND CATEGORIES\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Carica i dati dal file JSON originale\n",
    "with open(\"../features/dataset.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Leggi il file CSV\n",
    "csv_data = pd.read_csv('../features/contrastive_dataset.csv')\n",
    "\n",
    "# Lista per memorizzare le immagini con i dati combinati\n",
    "combined_images = []\n",
    "\n",
    "# Contatore per l'id delle immagini contrastive\n",
    "id_contrastive = 0\n",
    "\n",
    "# Per ogni riga nel file CSV\n",
    "for _, row in csv_data.iterrows():\n",
    "    # Trova l'immagine corrispondente nel file JSON utilizzando il nome del file come identificatore\n",
    "    matching_image = next((image for image in json_data[\"images\"] if image[\"file_name\"] == row[\"id_casi\"]), None)\n",
    "    # trova l'immagine corrispondente nel file JSON utilizzando il nome positivo come identificatore\n",
    "    matching_image_positive = next((image for image in json_data[\"images\"] if image[\"file_name\"] == row[\"positive\"]), None)\n",
    "    # trova l'immagine corrispondente nel file JSON utilizzando il nome negativo come identificatore\n",
    "    matching_image_negative = next((image for image in json_data[\"images\"] if image[\"file_name\"] == row[\"negative\"]), None)\n",
    "    if matching_image and matching_image_positive and matching_image_negative:\n",
    "        # Combina i dati dalle due fonti\n",
    "        combined_image = {\n",
    "            \"id_contrastive\": id_contrastive,\n",
    "            \"id\": matching_image[\"id\"],\n",
    "            \"id_casi\": row[\"id_casi\"],\n",
    "            \"positive\": row[\"positive\"],\n",
    "            \"negative\": row[\"negative\"],\n",
    "            \"category_id\": row[\"TIPO DI ULCERA\"],\n",
    "            \"dataset_id\": matching_image[\"dataset_id\"],\n",
    "            \"path\": matching_image[\"path\"],\n",
    "            \"width\": matching_image[\"width\"],\n",
    "            \"height\": matching_image[\"height\"],\n",
    "            \"file_name\": matching_image[\"file_name\"],\n",
    "            \"annotated\": matching_image[\"annotated\"],\n",
    "            \"annotating\": matching_image[\"annotating\"],\n",
    "            \"num_annotations\": matching_image[\"num_annotations\"],\n",
    "            \"metadata\": matching_image[\"metadata\"],\n",
    "            \"deleted\": matching_image[\"deleted\"],\n",
    "            \"milliseconds\": matching_image[\"milliseconds\"],\n",
    "            \"events\": matching_image[\"events\"],\n",
    "            \"regenerate_thumbnail\": matching_image[\"regenerate_thumbnail\"]\n",
    "        }\n",
    "        combined_images.append(combined_image)\n",
    "\n",
    "        # Incrementa il contatore per l'id delle immagini contrastive\n",
    "        id_contrastive += 1\n",
    "\n",
    "# Crea il JSON finale con la struttura richiesta\n",
    "final_json = {\"images\": combined_images}\n",
    "\n",
    "# Salva i dati combinati in un nuovo file JSON\n",
    "with open(\"../features/contrastive_dataset.json\", \"w\") as f:\n",
    "    json.dump(final_json, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il numero di immagini presenti nel file JSON è: 336\n",
      "Il numero di TRIPLETTE presenti nel file JSON è: 30359\n",
      "Il numero di annotazioni presenti nel file JSON è: 343\n"
     ]
    }
   ],
   "source": [
    "# conta il numero di immagini presenti nel file JSON\n",
    "import json\n",
    "\n",
    "# Carica i dati dal file JSON\n",
    "with open(\"../features/contrastive_dataset.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Conta il numero di immagini presenti nel file JSON\n",
    "num_images = len(json_data[\"images\"])\n",
    "\n",
    "# conta il numero di id unique presenti nel file JSON\n",
    "num_unique_images = len(set(image[\"id\"] for image in json_data[\"images\"]))\n",
    "print(f\"Il numero di immagini presenti nel file JSON è: {num_unique_images}\")\n",
    "\n",
    "# conta il numero di annotazioni presenti nel file JSON\n",
    "num_annotations = len(json_data[\"annotations\"])\n",
    "\n",
    "print(f\"Il numero di TRIPLETTE presenti nel file JSON è: {num_images}\")\n",
    "\n",
    "print(f\"Il numero di annotazioni presenti nel file JSON è: {num_annotations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
